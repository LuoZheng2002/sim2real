‚úèÔ∏è Setting installed package as editable
Initialized ACEBenchRunner with 9894 problems.
Acquiring build lock...
Building Rust extension with maturin develop...
Installed Rust extension successfully.
Released build lock.
use_api_for_all: False
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:28:31 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:28:31 [model.py:1545] Using max model len 40000
INFO 01-23 16:28:33 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 01-23 16:28:33 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 01-23 16:28:33 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
[0;36m(EngineCore_DP0 pid=1260189)[0;0m INFO 01-23 16:28:35 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1260189)[0;0m WARNING 01-23 16:28:35 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1260189)[0;0m INFO 01-23 16:28:41 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:46607 backend=nccl
[0;36m(EngineCore_DP0 pid=1260189)[0;0m INFO 01-23 16:28:41 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:46607 backend=nccl
[0;36m(EngineCore_DP0 pid=1260189)[0;0m INFO 01-23 16:28:42 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1260189)[0;0m WARNING 01-23 16:28:43 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1260189)[0;0m WARNING 01-23 16:28:43 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1260189)[0;0m INFO 01-23 16:28:43 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1260189)[0;0m INFO 01-23 16:28:43 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:28:51 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:28:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:29:43 [default_loader.py:291] Loading weights took 44.97 seconds
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:29:43 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 51.492808 seconds
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:30:04 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:30:04 [backends.py:704] Dynamo bytecode transform time: 20.05 s
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP1 pid=1260197)[0;0m INFO 01-23 16:30:21 [backends.py:261] Cache the graph of compile range (1, 2048) for later use
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:30:21 [backends.py:261] Cache the graph of compile range (1, 2048) for later use
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:30:43 [backends.py:278] Compiling a graph for compile range (1, 2048) takes 29.17 s
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:30:43 [monitor.py:34] torch.compile takes 49.22 s in total
[0;36m(EngineCore_DP0 pid=1260189)[0;0m INFO 01-23 16:30:43 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[0;36m(EngineCore_DP0 pid=1260189)[0;0m [0;36m(Worker_TP0 pid=1260195)[0;0m INFO 01-23 16:30:45 [gpu_worker.py:358] Available KV cache memory: 4.01 GiB
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:45 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.01 GiB). Based on the available memory, the estimated maximum model length is 32848. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1260189)[0;0m ERROR 01-23 16:30:46 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:30:47 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:30:47 [model.py:1545] Using max model len 40000
INFO 01-23 16:30:47 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1261877)[0;0m INFO 01-23 16:30:47 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1261877)[0;0m WARNING 01-23 16:30:47 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1261877)[0;0m INFO 01-23 16:30:52 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57739 backend=nccl
[0;36m(EngineCore_DP0 pid=1261877)[0;0m INFO 01-23 16:30:52 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57739 backend=nccl
[0;36m(EngineCore_DP0 pid=1261877)[0;0m INFO 01-23 16:30:52 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1261877)[0;0m WARNING 01-23 16:30:52 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1261877)[0;0m WARNING 01-23 16:30:52 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1261877)[0;0m INFO 01-23 16:30:52 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1261877)[0;0m INFO 01-23 16:30:52 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:30:55 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:30:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:31:45 [default_loader.py:291] Loading weights took 48.08 seconds
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:31:46 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 50.075406 seconds
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:32:05 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:32:05 [backends.py:704] Dynamo bytecode transform time: 19.03 s
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:32:23 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 9.347 s
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:32:23 [monitor.py:34] torch.compile takes 28.38 s in total
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP1 pid=1261885)[0;0m INFO 01-23 16:32:23 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 9.429 s
[0;36m(EngineCore_DP0 pid=1261877)[0;0m [0;36m(Worker_TP0 pid=1261883)[0;0m INFO 01-23 16:32:25 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:25 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1261877)[0;0m ERROR 01-23 16:32:26 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:32:27 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:32:27 [model.py:1545] Using max model len 40000
INFO 01-23 16:32:27 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1262729)[0;0m INFO 01-23 16:32:28 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1262729)[0;0m WARNING 01-23 16:32:28 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1262729)[0;0m INFO 01-23 16:32:32 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:33033 backend=nccl
[0;36m(EngineCore_DP0 pid=1262729)[0;0m INFO 01-23 16:32:32 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:33033 backend=nccl
[0;36m(EngineCore_DP0 pid=1262729)[0;0m INFO 01-23 16:32:32 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1262729)[0;0m WARNING 01-23 16:32:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1262729)[0;0m WARNING 01-23 16:32:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1262729)[0;0m INFO 01-23 16:32:33 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1262729)[0;0m INFO 01-23 16:32:33 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:32:35 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:32:36 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:33:22 [default_loader.py:291] Loading weights took 45.45 seconds
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:33:23 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 47.048868 seconds
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:33:43 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:33:43 [backends.py:704] Dynamo bytecode transform time: 19.53 s
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:34:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.753 s
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP1 pid=1262737)[0;0m INFO 01-23 16:34:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.786 s
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:34:00 [monitor.py:34] torch.compile takes 28.29 s in total
[0;36m(EngineCore_DP0 pid=1262729)[0;0m [0;36m(Worker_TP0 pid=1262735)[0;0m INFO 01-23 16:34:02 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:02 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1262729)[0;0m ERROR 01-23 16:34:03 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:34:04 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:34:04 [model.py:1545] Using max model len 40000
INFO 01-23 16:34:04 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1263579)[0;0m INFO 01-23 16:34:04 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1263579)[0;0m WARNING 01-23 16:34:04 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1263579)[0;0m INFO 01-23 16:34:10 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:49825 backend=nccl
[0;36m(EngineCore_DP0 pid=1263579)[0;0m INFO 01-23 16:34:10 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:49825 backend=nccl
[0;36m(EngineCore_DP0 pid=1263579)[0;0m INFO 01-23 16:34:10 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1263579)[0;0m WARNING 01-23 16:34:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1263579)[0;0m WARNING 01-23 16:34:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1263579)[0;0m INFO 01-23 16:34:10 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1263579)[0;0m INFO 01-23 16:34:10 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:34:12 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:34:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:34:58 [default_loader.py:291] Loading weights took 44.57 seconds
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:34:59 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.298742 seconds
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:35:19 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:35:19 [backends.py:704] Dynamo bytecode transform time: 19.55 s
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:35:37 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.828 s
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP1 pid=1263587)[0;0m INFO 01-23 16:35:37 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.891 s
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:35:37 [monitor.py:34] torch.compile takes 28.38 s in total
[0;36m(EngineCore_DP0 pid=1263579)[0;0m [0;36m(Worker_TP0 pid=1263585)[0;0m INFO 01-23 16:35:38 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:39 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1263579)[0;0m ERROR 01-23 16:35:40 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:35:40 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:35:40 [model.py:1545] Using max model len 40000
INFO 01-23 16:35:40 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1264089)[0;0m INFO 01-23 16:35:41 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1264089)[0;0m WARNING 01-23 16:35:41 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1264089)[0;0m INFO 01-23 16:35:45 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:42609 backend=nccl
[0;36m(EngineCore_DP0 pid=1264089)[0;0m INFO 01-23 16:35:45 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:42609 backend=nccl
[0;36m(EngineCore_DP0 pid=1264089)[0;0m INFO 01-23 16:35:45 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1264089)[0;0m WARNING 01-23 16:35:46 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1264089)[0;0m WARNING 01-23 16:35:46 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1264089)[0;0m INFO 01-23 16:35:46 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1264089)[0;0m INFO 01-23 16:35:46 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:35:48 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:35:49 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:36:35 [default_loader.py:291] Loading weights took 45.12 seconds
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:36:35 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.664852 seconds
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:36:55 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:36:55 [backends.py:704] Dynamo bytecode transform time: 19.64 s
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:37:13 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.823 s
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP1 pid=1264097)[0;0m INFO 01-23 16:37:13 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.893 s
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:37:13 [monitor.py:34] torch.compile takes 28.46 s in total
[0;36m(EngineCore_DP0 pid=1264089)[0;0m [0;36m(Worker_TP0 pid=1264095)[0;0m INFO 01-23 16:37:15 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:15 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1264089)[0;0m ERROR 01-23 16:37:16 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:37:17 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:37:17 [model.py:1545] Using max model len 40000
INFO 01-23 16:37:17 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1264946)[0;0m INFO 01-23 16:37:17 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1264946)[0;0m WARNING 01-23 16:37:17 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1264946)[0;0m INFO 01-23 16:37:22 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:35937 backend=nccl
[0;36m(EngineCore_DP0 pid=1264946)[0;0m INFO 01-23 16:37:22 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35937 backend=nccl
[0;36m(EngineCore_DP0 pid=1264946)[0;0m INFO 01-23 16:37:22 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1264946)[0;0m WARNING 01-23 16:37:22 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1264946)[0;0m WARNING 01-23 16:37:22 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1264946)[0;0m INFO 01-23 16:37:22 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1264946)[0;0m INFO 01-23 16:37:22 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:37:24 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:37:25 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:38:11 [default_loader.py:291] Loading weights took 44.75 seconds
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:38:11 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.172069 seconds
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:38:31 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:38:31 [backends.py:704] Dynamo bytecode transform time: 19.25 s
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:38:49 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.770 s
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP1 pid=1264954)[0;0m INFO 01-23 16:38:49 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.822 s
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:38:49 [monitor.py:34] torch.compile takes 28.02 s in total
[0;36m(EngineCore_DP0 pid=1264946)[0;0m [0;36m(Worker_TP0 pid=1264952)[0;0m INFO 01-23 16:38:51 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:51 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1264946)[0;0m ERROR 01-23 16:38:52 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:38:53 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:38:53 [model.py:1545] Using max model len 40000
INFO 01-23 16:38:53 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1265454)[0;0m INFO 01-23 16:38:53 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1265454)[0;0m WARNING 01-23 16:38:53 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1265454)[0;0m INFO 01-23 16:38:58 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:52081 backend=nccl
[0;36m(EngineCore_DP0 pid=1265454)[0;0m INFO 01-23 16:38:58 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:52081 backend=nccl
[0;36m(EngineCore_DP0 pid=1265454)[0;0m INFO 01-23 16:38:58 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1265454)[0;0m WARNING 01-23 16:38:58 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1265454)[0;0m WARNING 01-23 16:38:58 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1265454)[0;0m INFO 01-23 16:38:58 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1265454)[0;0m INFO 01-23 16:38:58 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:39:00 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:39:01 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:39:47 [default_loader.py:291] Loading weights took 44.81 seconds
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:39:47 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.482502 seconds
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:40:07 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:40:07 [backends.py:704] Dynamo bytecode transform time: 19.28 s
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:40:24 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.259 s
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP1 pid=1265462)[0;0m INFO 01-23 16:40:24 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.330 s
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:40:24 [monitor.py:34] torch.compile takes 27.54 s in total
[0;36m(EngineCore_DP0 pid=1265454)[0;0m [0;36m(Worker_TP0 pid=1265460)[0;0m INFO 01-23 16:40:26 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:27 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1265454)[0;0m ERROR 01-23 16:40:28 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:40:28 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:40:28 [model.py:1545] Using max model len 40000
INFO 01-23 16:40:28 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1266336)[0;0m INFO 01-23 16:40:29 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1266336)[0;0m WARNING 01-23 16:40:29 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1266336)[0;0m INFO 01-23 16:40:34 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:56295 backend=nccl
[0;36m(EngineCore_DP0 pid=1266336)[0;0m INFO 01-23 16:40:34 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:56295 backend=nccl
[0;36m(EngineCore_DP0 pid=1266336)[0;0m INFO 01-23 16:40:34 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1266336)[0;0m WARNING 01-23 16:40:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1266336)[0;0m WARNING 01-23 16:40:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1266336)[0;0m INFO 01-23 16:40:35 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1266336)[0;0m INFO 01-23 16:40:35 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:40:37 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:40:38 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:41:23 [default_loader.py:291] Loading weights took 45.08 seconds
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:41:24 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.553488 seconds
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:41:44 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:41:44 [backends.py:704] Dynamo bytecode transform time: 19.24 s
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:42:02 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.690 s
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP1 pid=1266344)[0;0m INFO 01-23 16:42:02 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.729 s
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:42:02 [monitor.py:34] torch.compile takes 27.93 s in total
[0;36m(EngineCore_DP0 pid=1266336)[0;0m [0;36m(Worker_TP0 pid=1266342)[0;0m INFO 01-23 16:42:03 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:03 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1266336)[0;0m ERROR 01-23 16:42:04 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:42:05 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:42:05 [model.py:1545] Using max model len 40000
INFO 01-23 16:42:05 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1267186)[0;0m INFO 01-23 16:42:05 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1267186)[0;0m WARNING 01-23 16:42:05 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1267186)[0;0m INFO 01-23 16:42:10 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:52579 backend=nccl
[0;36m(EngineCore_DP0 pid=1267186)[0;0m INFO 01-23 16:42:10 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:52579 backend=nccl
[0;36m(EngineCore_DP0 pid=1267186)[0;0m INFO 01-23 16:42:10 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1267186)[0;0m WARNING 01-23 16:42:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1267186)[0;0m WARNING 01-23 16:42:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1267186)[0;0m INFO 01-23 16:42:10 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1267186)[0;0m INFO 01-23 16:42:10 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:42:12 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:42:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:42:59 [default_loader.py:291] Loading weights took 45.15 seconds
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:43:00 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.561176 seconds
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:43:20 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:43:20 [backends.py:704] Dynamo bytecode transform time: 19.52 s
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP1 pid=1267194)[0;0m INFO 01-23 16:43:37 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.763 s
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:43:37 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.521 s
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:43:37 [monitor.py:34] torch.compile takes 28.04 s in total
[0;36m(EngineCore_DP0 pid=1267186)[0;0m [0;36m(Worker_TP0 pid=1267192)[0;0m INFO 01-23 16:43:39 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:39 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1267186)[0;0m ERROR 01-23 16:43:40 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:43:41 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:43:41 [model.py:1545] Using max model len 40000
INFO 01-23 16:43:41 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1267694)[0;0m INFO 01-23 16:43:41 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1267694)[0;0m WARNING 01-23 16:43:41 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1267694)[0;0m INFO 01-23 16:43:46 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:36485 backend=nccl
[0;36m(EngineCore_DP0 pid=1267694)[0;0m INFO 01-23 16:43:46 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:36485 backend=nccl
[0;36m(EngineCore_DP0 pid=1267694)[0;0m INFO 01-23 16:43:46 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1267694)[0;0m WARNING 01-23 16:43:46 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1267694)[0;0m WARNING 01-23 16:43:46 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1267694)[0;0m INFO 01-23 16:43:46 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1267694)[0;0m INFO 01-23 16:43:46 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:43:48 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:43:49 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:44:35 [default_loader.py:291] Loading weights took 45.23 seconds
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:44:36 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.616144 seconds
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:44:55 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:44:55 [backends.py:704] Dynamo bytecode transform time: 19.07 s
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP1 pid=1267702)[0;0m INFO 01-23 16:45:12 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.300 s
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:45:12 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.234 s
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:45:12 [monitor.py:34] torch.compile takes 27.31 s in total
[0;36m(EngineCore_DP0 pid=1267694)[0;0m [0;36m(Worker_TP0 pid=1267700)[0;0m INFO 01-23 16:45:14 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1267694)[0;0m ERROR 01-23 16:45:15 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:45:16 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:45:16 [model.py:1545] Using max model len 40000
INFO 01-23 16:45:16 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1268548)[0;0m INFO 01-23 16:45:16 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1268548)[0;0m WARNING 01-23 16:45:16 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1268548)[0;0m INFO 01-23 16:45:21 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:53465 backend=nccl
[0;36m(EngineCore_DP0 pid=1268548)[0;0m INFO 01-23 16:45:21 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:53465 backend=nccl
[0;36m(EngineCore_DP0 pid=1268548)[0;0m INFO 01-23 16:45:21 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1268548)[0;0m WARNING 01-23 16:45:22 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1268548)[0;0m WARNING 01-23 16:45:22 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1268548)[0;0m INFO 01-23 16:45:22 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1268548)[0;0m INFO 01-23 16:45:22 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:45:24 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:45:25 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:46:14 [default_loader.py:291] Loading weights took 48.25 seconds
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:46:14 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 49.802517 seconds
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:46:34 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:46:34 [backends.py:704] Dynamo bytecode transform time: 19.30 s
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP1 pid=1268556)[0;0m INFO 01-23 16:46:51 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.387 s
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:46:51 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.404 s
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:46:51 [monitor.py:34] torch.compile takes 27.71 s in total
[0;36m(EngineCore_DP0 pid=1268548)[0;0m [0;36m(Worker_TP0 pid=1268554)[0;0m INFO 01-23 16:46:53 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:53 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1268548)[0;0m ERROR 01-23 16:46:54 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:46:55 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:46:55 [model.py:1545] Using max model len 40000
INFO 01-23 16:46:55 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1269056)[0;0m INFO 01-23 16:46:55 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1269056)[0;0m WARNING 01-23 16:46:55 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1269056)[0;0m INFO 01-23 16:47:00 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:44415 backend=nccl
[0;36m(EngineCore_DP0 pid=1269056)[0;0m INFO 01-23 16:47:00 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:44415 backend=nccl
[0;36m(EngineCore_DP0 pid=1269056)[0;0m INFO 01-23 16:47:01 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1269056)[0;0m WARNING 01-23 16:47:01 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1269056)[0;0m WARNING 01-23 16:47:01 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1269056)[0;0m INFO 01-23 16:47:01 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1269056)[0;0m INFO 01-23 16:47:01 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:47:03 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:47:04 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:47:54 [default_loader.py:291] Loading weights took 49.32 seconds
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:47:55 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 50.879932 seconds
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:48:14 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:48:14 [backends.py:704] Dynamo bytecode transform time: 19.45 s
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:48:32 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.204 s
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP1 pid=1269064)[0;0m INFO 01-23 16:48:32 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.306 s
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:48:32 [monitor.py:34] torch.compile takes 27.65 s in total
[0;36m(EngineCore_DP0 pid=1269056)[0;0m [0;36m(Worker_TP0 pid=1269062)[0;0m INFO 01-23 16:48:33 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:33 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1269056)[0;0m ERROR 01-23 16:48:34 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
Created API backend for model gpt-4.1
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:48:37 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:48:37 [model.py:1545] Using max model len 40000
INFO 01-23 16:48:37 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1269911)[0;0m INFO 01-23 16:48:37 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1269911)[0;0m WARNING 01-23 16:48:37 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1269911)[0;0m INFO 01-23 16:48:42 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:38423 backend=nccl
[0;36m(EngineCore_DP0 pid=1269911)[0;0m INFO 01-23 16:48:42 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:38423 backend=nccl
[0;36m(EngineCore_DP0 pid=1269911)[0;0m INFO 01-23 16:48:42 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1269911)[0;0m WARNING 01-23 16:48:43 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1269911)[0;0m WARNING 01-23 16:48:43 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1269911)[0;0m INFO 01-23 16:48:43 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1269911)[0;0m INFO 01-23 16:48:43 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:48:45 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:48:46 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:49:33 [default_loader.py:291] Loading weights took 46.53 seconds
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:49:34 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 47.939307 seconds
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:49:53 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:49:53 [backends.py:704] Dynamo bytecode transform time: 19.51 s
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:50:11 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.234 s
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:50:11 [monitor.py:34] torch.compile takes 27.74 s in total
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP1 pid=1269919)[0;0m INFO 01-23 16:50:11 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.281 s
[0;36m(EngineCore_DP0 pid=1269911)[0;0m [0;36m(Worker_TP0 pid=1269917)[0;0m INFO 01-23 16:50:12 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1269911)[0;0m ERROR 01-23 16:50:13 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:50:14 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:50:14 [model.py:1545] Using max model len 40000
INFO 01-23 16:50:14 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1270793)[0;0m INFO 01-23 16:50:14 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1270793)[0;0m WARNING 01-23 16:50:14 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1270793)[0;0m INFO 01-23 16:50:19 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:42733 backend=nccl
[0;36m(EngineCore_DP0 pid=1270793)[0;0m INFO 01-23 16:50:19 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:42733 backend=nccl
[0;36m(EngineCore_DP0 pid=1270793)[0;0m INFO 01-23 16:50:19 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1270793)[0;0m WARNING 01-23 16:50:19 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1270793)[0;0m WARNING 01-23 16:50:19 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1270793)[0;0m INFO 01-23 16:50:19 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1270793)[0;0m INFO 01-23 16:50:19 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:50:21 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:50:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:51:08 [default_loader.py:291] Loading weights took 44.57 seconds
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:51:08 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.126505 seconds
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:51:28 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:51:28 [backends.py:704] Dynamo bytecode transform time: 19.38 s
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP1 pid=1270801)[0;0m INFO 01-23 16:51:46 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.290 s
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:51:46 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.205 s
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:51:46 [monitor.py:34] torch.compile takes 27.58 s in total
[0;36m(EngineCore_DP0 pid=1270793)[0;0m [0;36m(Worker_TP0 pid=1270799)[0;0m INFO 01-23 16:51:47 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:47 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1270793)[0;0m ERROR 01-23 16:51:48 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:51:49 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:51:49 [model.py:1545] Using max model len 40000
INFO 01-23 16:51:49 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1271301)[0;0m INFO 01-23 16:51:49 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1271301)[0;0m WARNING 01-23 16:51:49 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1271301)[0;0m INFO 01-23 16:51:54 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:43093 backend=nccl
[0;36m(EngineCore_DP0 pid=1271301)[0;0m INFO 01-23 16:51:54 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:43093 backend=nccl
[0;36m(EngineCore_DP0 pid=1271301)[0;0m INFO 01-23 16:51:54 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1271301)[0;0m WARNING 01-23 16:51:54 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1271301)[0;0m WARNING 01-23 16:51:54 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1271301)[0;0m INFO 01-23 16:51:54 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1271301)[0;0m INFO 01-23 16:51:54 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:51:56 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:51:57 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:52:43 [default_loader.py:291] Loading weights took 44.90 seconds
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:52:43 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.256487 seconds
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:53:04 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:53:04 [backends.py:704] Dynamo bytecode transform time: 19.87 s
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP1 pid=1271309)[0;0m INFO 01-23 16:53:21 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.672 s
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:53:21 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.617 s
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:53:21 [monitor.py:34] torch.compile takes 28.49 s in total
[0;36m(EngineCore_DP0 pid=1271301)[0;0m [0;36m(Worker_TP0 pid=1271307)[0;0m INFO 01-23 16:53:23 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:23 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1271301)[0;0m ERROR 01-23 16:53:24 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:53:25 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:53:25 [model.py:1545] Using max model len 40000
INFO 01-23 16:53:25 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1272152)[0;0m INFO 01-23 16:53:25 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1272152)[0;0m WARNING 01-23 16:53:25 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1272152)[0;0m INFO 01-23 16:53:30 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:49893 backend=nccl
[0;36m(EngineCore_DP0 pid=1272152)[0;0m INFO 01-23 16:53:30 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:49893 backend=nccl
[0;36m(EngineCore_DP0 pid=1272152)[0;0m INFO 01-23 16:53:30 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1272152)[0;0m WARNING 01-23 16:53:30 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1272152)[0;0m WARNING 01-23 16:53:30 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1272152)[0;0m INFO 01-23 16:53:31 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1272152)[0;0m INFO 01-23 16:53:31 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:53:33 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:53:34 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:54:19 [default_loader.py:291] Loading weights took 45.10 seconds
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:54:20 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.473676 seconds
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:54:40 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:54:40 [backends.py:704] Dynamo bytecode transform time: 19.33 s
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:54:57 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.103 s
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:54:57 [monitor.py:34] torch.compile takes 27.43 s in total
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP1 pid=1272160)[0;0m INFO 01-23 16:54:57 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.201 s
[0;36m(EngineCore_DP0 pid=1272152)[0;0m [0;36m(Worker_TP0 pid=1272158)[0;0m INFO 01-23 16:54:59 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:54:59 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1272152)[0;0m ERROR 01-23 16:55:00 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:55:01 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:55:01 [model.py:1545] Using max model len 40000
INFO 01-23 16:55:01 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1273005)[0;0m INFO 01-23 16:55:01 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1273005)[0;0m WARNING 01-23 16:55:01 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1273005)[0;0m INFO 01-23 16:55:06 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:46101 backend=nccl
[0;36m(EngineCore_DP0 pid=1273005)[0;0m INFO 01-23 16:55:06 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:46101 backend=nccl
[0;36m(EngineCore_DP0 pid=1273005)[0;0m INFO 01-23 16:55:06 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1273005)[0;0m WARNING 01-23 16:55:06 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1273005)[0;0m WARNING 01-23 16:55:06 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1273005)[0;0m INFO 01-23 16:55:06 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1273005)[0;0m INFO 01-23 16:55:06 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:55:08 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:55:09 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:55:55 [default_loader.py:291] Loading weights took 44.79 seconds
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:55:56 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.329490 seconds
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:56:16 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:56:16 [backends.py:704] Dynamo bytecode transform time: 20.05 s
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:56:33 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.141 s
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:56:33 [monitor.py:34] torch.compile takes 28.19 s in total
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP1 pid=1273013)[0;0m INFO 01-23 16:56:33 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.258 s
[0;36m(EngineCore_DP0 pid=1273005)[0;0m [0;36m(Worker_TP0 pid=1273011)[0;0m INFO 01-23 16:56:35 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1273005)[0;0m ERROR 01-23 16:56:36 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:56:37 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:56:37 [model.py:1545] Using max model len 40000
INFO 01-23 16:56:37 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1273514)[0;0m INFO 01-23 16:56:37 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1273514)[0;0m WARNING 01-23 16:56:37 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1273514)[0;0m INFO 01-23 16:56:42 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:45807 backend=nccl
[0;36m(EngineCore_DP0 pid=1273514)[0;0m INFO 01-23 16:56:42 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:45807 backend=nccl
[0;36m(EngineCore_DP0 pid=1273514)[0;0m INFO 01-23 16:56:42 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1273514)[0;0m WARNING 01-23 16:56:43 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1273514)[0;0m WARNING 01-23 16:56:43 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1273514)[0;0m INFO 01-23 16:56:43 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1273514)[0;0m INFO 01-23 16:56:43 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:56:45 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:56:46 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:57:31 [default_loader.py:291] Loading weights took 44.85 seconds
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:57:32 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.384744 seconds
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:57:52 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:57:52 [backends.py:704] Dynamo bytecode transform time: 19.40 s
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:58:09 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.190 s
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP1 pid=1273522)[0;0m INFO 01-23 16:58:09 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.214 s
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:58:09 [monitor.py:34] torch.compile takes 27.59 s in total
[0;36m(EngineCore_DP0 pid=1273514)[0;0m [0;36m(Worker_TP0 pid=1273520)[0;0m INFO 01-23 16:58:11 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1273514)[0;0m ERROR 01-23 16:58:12 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:58:13 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:58:13 [model.py:1545] Using max model len 40000
INFO 01-23 16:58:13 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1274366)[0;0m INFO 01-23 16:58:13 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1274366)[0;0m WARNING 01-23 16:58:13 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1274366)[0;0m INFO 01-23 16:58:18 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:58253 backend=nccl
[0;36m(EngineCore_DP0 pid=1274366)[0;0m INFO 01-23 16:58:18 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:58253 backend=nccl
[0;36m(EngineCore_DP0 pid=1274366)[0;0m INFO 01-23 16:58:18 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1274366)[0;0m WARNING 01-23 16:58:19 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1274366)[0;0m WARNING 01-23 16:58:19 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1274366)[0;0m INFO 01-23 16:58:19 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1274366)[0;0m INFO 01-23 16:58:19 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:58:21 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:58:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:59:07 [default_loader.py:291] Loading weights took 44.71 seconds
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:59:08 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.245590 seconds
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:59:28 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:59:28 [backends.py:704] Dynamo bytecode transform time: 19.33 s
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:59:45 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.208 s
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP1 pid=1274374)[0;0m INFO 01-23 16:59:45 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.213 s
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:59:45 [monitor.py:34] torch.compile takes 27.54 s in total
[0;36m(EngineCore_DP0 pid=1274366)[0;0m [0;36m(Worker_TP0 pid=1274372)[0;0m INFO 01-23 16:59:47 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:47 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1274366)[0;0m ERROR 01-23 16:59:48 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 16:59:49 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 16:59:49 [model.py:1545] Using max model len 40000
INFO 01-23 16:59:49 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1274874)[0;0m INFO 01-23 16:59:49 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1274874)[0;0m WARNING 01-23 16:59:49 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1274874)[0;0m INFO 01-23 16:59:54 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34955 backend=nccl
[0;36m(EngineCore_DP0 pid=1274874)[0;0m INFO 01-23 16:59:54 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34955 backend=nccl
[0;36m(EngineCore_DP0 pid=1274874)[0;0m INFO 01-23 16:59:54 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1274874)[0;0m WARNING 01-23 16:59:54 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1274874)[0;0m WARNING 01-23 16:59:54 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1274874)[0;0m INFO 01-23 16:59:54 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1274874)[0;0m INFO 01-23 16:59:54 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 16:59:56 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 16:59:57 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 17:00:45 [default_loader.py:291] Loading weights took 46.96 seconds
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 17:00:46 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 48.721466 seconds
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 17:01:05 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 17:01:05 [backends.py:704] Dynamo bytecode transform time: 19.37 s
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 17:01:23 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.227 s
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 17:01:23 [monitor.py:34] torch.compile takes 27.59 s in total
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP1 pid=1274882)[0;0m INFO 01-23 17:01:23 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.257 s
[0;36m(EngineCore_DP0 pid=1274874)[0;0m [0;36m(Worker_TP0 pid=1274880)[0;0m INFO 01-23 17:01:25 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:25 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1274874)[0;0m ERROR 01-23 17:01:26 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 17:01:27 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 17:01:27 [model.py:1545] Using max model len 40000
INFO 01-23 17:01:27 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1275773)[0;0m INFO 01-23 17:01:27 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1275773)[0;0m WARNING 01-23 17:01:27 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1275773)[0;0m INFO 01-23 17:01:32 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57907 backend=nccl
[0;36m(EngineCore_DP0 pid=1275773)[0;0m INFO 01-23 17:01:32 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57907 backend=nccl
[0;36m(EngineCore_DP0 pid=1275773)[0;0m INFO 01-23 17:01:32 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1275773)[0;0m WARNING 01-23 17:01:32 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1275773)[0;0m WARNING 01-23 17:01:32 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1275773)[0;0m INFO 01-23 17:01:32 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1275773)[0;0m INFO 01-23 17:01:32 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:01:34 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:01:35 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:02:24 [default_loader.py:291] Loading weights took 48.09 seconds
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:02:25 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 49.551262 seconds
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:02:45 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:02:45 [backends.py:704] Dynamo bytecode transform time: 19.63 s
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:03:03 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.735 s
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP1 pid=1275781)[0;0m INFO 01-23 17:03:03 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.773 s
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:03:03 [monitor.py:34] torch.compile takes 28.36 s in total
[0;36m(EngineCore_DP0 pid=1275773)[0;0m [0;36m(Worker_TP0 pid=1275779)[0;0m INFO 01-23 17:03:04 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:05 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1275773)[0;0m ERROR 01-23 17:03:06 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 17:03:06 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 17:03:06 [model.py:1545] Using max model len 40000
INFO 01-23 17:03:06 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1276624)[0;0m INFO 01-23 17:03:07 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1276624)[0;0m WARNING 01-23 17:03:07 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1276624)[0;0m INFO 01-23 17:03:12 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35627 backend=nccl
[0;36m(EngineCore_DP0 pid=1276624)[0;0m INFO 01-23 17:03:12 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:35627 backend=nccl
[0;36m(EngineCore_DP0 pid=1276624)[0;0m INFO 01-23 17:03:12 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1276624)[0;0m WARNING 01-23 17:03:12 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1276624)[0;0m WARNING 01-23 17:03:12 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1276624)[0;0m INFO 01-23 17:03:12 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1276624)[0;0m INFO 01-23 17:03:12 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:03:14 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:03:15 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:04:01 [default_loader.py:291] Loading weights took 45.02 seconds
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:04:01 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 46.401134 seconds
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:04:21 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:04:21 [backends.py:704] Dynamo bytecode transform time: 19.29 s
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:04:38 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.094 s
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP1 pid=1276632)[0;0m INFO 01-23 17:04:38 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.193 s
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:04:38 [monitor.py:34] torch.compile takes 27.39 s in total
[0;36m(EngineCore_DP0 pid=1276624)[0;0m [0;36m(Worker_TP0 pid=1276630)[0;0m INFO 01-23 17:04:40 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1276624)[0;0m ERROR 01-23 17:04:41 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 17:04:42 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 17:04:42 [model.py:1545] Using max model len 40000
INFO 01-23 17:04:42 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1277132)[0;0m INFO 01-23 17:04:42 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1277132)[0;0m WARNING 01-23 17:04:42 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1277132)[0;0m INFO 01-23 17:04:47 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:47879 backend=nccl
[0;36m(EngineCore_DP0 pid=1277132)[0;0m INFO 01-23 17:04:47 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:47879 backend=nccl
[0;36m(EngineCore_DP0 pid=1277132)[0;0m INFO 01-23 17:04:47 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1277132)[0;0m WARNING 01-23 17:04:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1277132)[0;0m WARNING 01-23 17:04:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1277132)[0;0m INFO 01-23 17:04:47 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1277132)[0;0m INFO 01-23 17:04:47 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:04:49 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:04:51 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:05:35 [default_loader.py:291] Loading weights took 43.75 seconds
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:05:36 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 45.483977 seconds
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:05:55 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:05:55 [backends.py:704] Dynamo bytecode transform time: 19.21 s
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:06:13 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.222 s
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:06:13 [monitor.py:34] torch.compile takes 27.43 s in total
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP1 pid=1277140)[0;0m INFO 01-23 17:06:13 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.267 s
[0;36m(EngineCore_DP0 pid=1277132)[0;0m [0;36m(Worker_TP0 pid=1277138)[0;0m INFO 01-23 17:06:14 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:14 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1277132)[0;0m ERROR 01-23 17:06:15 [multiproc_executor.py:231] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 17:06:16 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 17:06:16 [model.py:1545] Using max model len 40000
INFO 01-23 17:06:16 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1278002)[0;0m INFO 01-23 17:06:16 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1278002)[0;0m WARNING 01-23 17:06:16 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1278002)[0;0m INFO 01-23 17:06:21 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:38425 backend=nccl
[0;36m(EngineCore_DP0 pid=1278002)[0;0m INFO 01-23 17:06:21 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:38425 backend=nccl
[0;36m(EngineCore_DP0 pid=1278002)[0;0m INFO 01-23 17:06:21 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1278002)[0;0m WARNING 01-23 17:06:21 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1278002)[0;0m WARNING 01-23 17:06:21 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1278002)[0;0m INFO 01-23 17:06:21 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1278002)[0;0m INFO 01-23 17:06:21 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:06:23 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:06:24 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:07:10 [default_loader.py:291] Loading weights took 45.86 seconds
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:07:11 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 47.263902 seconds
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:07:31 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:07:31 [backends.py:704] Dynamo bytecode transform time: 19.19 s
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:07:48 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.156 s
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:07:48 [monitor.py:34] torch.compile takes 27.34 s in total
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP1 pid=1278010)[0;0m INFO 01-23 17:07:48 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.203 s
[0;36m(EngineCore_DP0 pid=1278002)[0;0m [0;36m(Worker_TP0 pid=1278008)[0;0m INFO 01-23 17:07:49 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1278002)[0;0m ERROR 01-23 17:07:50 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 17:07:51 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 17:07:51 [model.py:1545] Using max model len 40000
INFO 01-23 17:07:51 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1278509)[0;0m INFO 01-23 17:07:51 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1278509)[0;0m WARNING 01-23 17:07:51 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1278509)[0;0m INFO 01-23 17:07:56 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57113 backend=nccl
[0;36m(EngineCore_DP0 pid=1278509)[0;0m INFO 01-23 17:07:56 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57113 backend=nccl
[0;36m(EngineCore_DP0 pid=1278509)[0;0m INFO 01-23 17:07:56 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1278509)[0;0m WARNING 01-23 17:07:57 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1278509)[0;0m WARNING 01-23 17:07:57 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1278509)[0;0m INFO 01-23 17:07:57 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1278509)[0;0m INFO 01-23 17:07:57 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:07:58 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:07:59 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:08:46 [default_loader.py:291] Loading weights took 46.14 seconds
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:08:47 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 47.806733 seconds
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:09:07 [backends.py:644] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/20ae177596/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:09:07 [backends.py:704] Dynamo bytecode transform time: 19.69 s
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:09:24 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.224 s
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP1 pid=1278517)[0;0m INFO 01-23 17:09:24 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 8.247 s
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:09:24 [monitor.py:34] torch.compile takes 27.91 s in total
[0;36m(EngineCore_DP0 pid=1278509)[0;0m [0;36m(Worker_TP0 pid=1278515)[0;0m INFO 01-23 17:09:26 [gpu_worker.py:358] Available KV cache memory: 4.08 GiB
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         vllm_config,
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 113, in __init__
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         vllm_config
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 254, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         vllm_config, kv_cache_specs, available_gpu_memory
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 1514, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         min(available_memory),
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ...<6 lines>...
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         ),
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]         ^^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ^
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]   File "/projects/bfdz/zluo8/sim2real/.venv/lib/python3.13/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     ...<8 lines>...
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936]     )
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:26 [core.py:936] ValueError: To serve at least one request with the models's max seq len (40000), (4.88 GiB KV cache is needed, which is larger than the available KV cache memory (4.08 GiB). Based on the available memory, the estimated maximum model length is 33376. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=1278509)[0;0m ERROR 01-23 17:09:27 [multiproc_executor.py:231] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
importing vllm and transformers...
vllm and transformers imported.
Creating vLLM backend...
INFO 01-23 17:09:28 [model.py:530] Resolved architecture: Qwen3ForCausalLM
INFO 01-23 17:09:28 [model.py:1545] Using max model len 40000
INFO 01-23 17:09:28 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1279361)[0;0m INFO 01-23 17:09:28 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='zzwkk/MUA-RL-32B', speculative_config=None, tokenizer='zzwkk/MUA-RL-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=zzwkk/MUA-RL-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1279361)[0;0m WARNING 01-23 17:09:28 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1279361)[0;0m INFO 01-23 17:09:33 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:42903 backend=nccl
[0;36m(EngineCore_DP0 pid=1279361)[0;0m INFO 01-23 17:09:33 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:42903 backend=nccl
[0;36m(EngineCore_DP0 pid=1279361)[0;0m INFO 01-23 17:09:33 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1279361)[0;0m WARNING 01-23 17:09:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1279361)[0;0m WARNING 01-23 17:09:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=1279361)[0;0m INFO 01-23 17:09:33 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=1279361)[0;0m INFO 01-23 17:09:33 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(EngineCore_DP0 pid=1279361)[0;0m [0;36m(Worker_TP0 pid=1279367)[0;0m INFO 01-23 17:09:35 [gpu_model_runner.py:3808] Starting to load model zzwkk/MUA-RL-32B...
[0;36m(EngineCore_DP0 pid=1279361)[0;0m [0;36m(Worker_TP0 pid=1279367)[0;0m INFO 01-23 17:09:36 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1279361)[0;0m [0;36m(Worker_TP0 pid=1279367)[0;0m INFO 01-23 17:10:23 [default_loader.py:291] Loading weights took 46.59 seconds
[0;36m(EngineCore_DP0 pid=1279361)[0;0m [0;36m(Worker_TP0 pid=1279367)[0;0m INFO 01-23 17:10:24 [gpu_model_runner.py:3905] Model loading took 30.59 GiB memory and 47.954559 seconds
